# ğŸ“„ ZEDIA (PDF RAG Chatbot)

A CLI-based Retrieval-Augmented Generation (RAG) system that lets you **upload PDFs**, extract & chunk their text, embed into a **Chroma vector store**, and query them using **OPEN AI**.

---

## ğŸš€ Features
- ğŸ“¥ **Extract text from PDFs** using [Unstract LLM Whisperer](https://docs.unstract.com/).
- âœ‚ï¸ **Split documents into chunks** with LangChainâ€™s `RecursiveCharacterTextSplitter`.
- ğŸ§  **Embed text** using `OpenAIEmbeddings` and store in ChromaDB.
- ğŸ¤– **Query PDFs** interactively with GPT-4.
- ğŸ—„ **Persist embeddings** for fast retrieval across sessions.
- ğŸ’» **CLI commands** for reading, querying, and managing PDFs.

---

## ğŸ“¦ Project Structure

```
.
â”œâ”€â”€ main.py                     # CLI entry point
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ pdf_loader.py           # Extracts & splits PDF text
â”‚   â”œâ”€â”€ vector_store.py         # Saves/loads Chroma vector store
â”‚   â”œâ”€â”€ rag_chain.py            # Creates RetrievalQA chain
â”œâ”€â”€ pdfs/                       # Folder containing PDFs to read
â”œâ”€â”€ vector_store/               # ChromaDB storage
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ .env                        # API keys & environment variables
```

---

## âš™ï¸ Installation

1ï¸âƒ£ **Clone the repo**
```bash
git clone https://github.com/mhmdmnthsr/Zedia.git
cd Zedia
```

2ï¸âƒ£ **Create a virtual environment**
```bash
python -m venv venv
source venv/bin/activate    # Mac/Linux
venv\Scripts\activate       # Windows
```

4ï¸âƒ£ **Set up environment variables**

Create a `.env` file:
```ini
OPENAI_API_KEY=your_openai_api_key
LLM_WHISPERER_API_KEY=your_llmwhisperer_api_key
```

---

## ğŸ“œ Usage

### **1. Add PDFs**
Place all PDFs you want to query inside the `pdfs/` folder.

### **2. Read & Embed PDFs**
```bash
python main.py read
```
- Extracts text from all PDFs in `pdfs/`
- Splits text into chunks
- Saves chunks into ChromaDB

### **3. Query PDFs**
```bash
python main.py query
```
Example:
```
ğŸ¤– What do you want from the PDF (type 'exit' to quit):
You: What is the main conclusion of the report?
ğŸ§  Answer:
The report concludes that...
ğŸ“š Sources:
report.pdf
```

---

## ğŸ›  Commands

| Command | Description |
|---------|-------------|
| `python main.py read` | Extracts & embeds all PDFs from the `pdfs/` folder |
| `python main.py query` | Starts interactive chat to query embedded PDFs |
| `python main.py list` | Lists all PDFs that have been embedded |

---

## ğŸ§© Tech Stack
- **[LangChain](https://www.langchain.com/)** â€” Text splitting, document handling, and RAG chain
- **[ChromaDB](https://www.trychroma.com/)** â€” Vector database for embeddings
- **[OpenAI GPT-4](https://platform.openai.com/)** â€” Answer generation
- **[Unstract LLM Whisperer](https://docs.unstract.com/)** â€” PDF text extraction

---

## ğŸ“Œ Notes
- All PDFs are processed fresh every time 
- ChromaDB persists embeddings in `vector_store/` for faster queries.
- Adjust `chunk_size` and `chunk_overlap` in `pdf_loader.py` for different document sizes.
